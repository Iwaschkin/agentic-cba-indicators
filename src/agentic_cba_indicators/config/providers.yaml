# =============================================================================
# Model Provider Configuration
# =============================================================================
# This file configures which AI model provider to use for the Strands agent.
# Supported providers: ollama, anthropic, openai, bedrock, gemini
#
# Environment variables can be used for sensitive values:
#   ${ANTHROPIC_API_KEY} - will read from environment variable
# =============================================================================

# Active provider (choose one: ollama, anthropic, openai, bedrock, gemini)
active_provider: ollama

# =============================================================================
# Provider Configurations
# =============================================================================

providers:
  # ---------------------------------------------------------------------------
  # Ollama - Local LLM (default)
  # ---------------------------------------------------------------------------
  ollama:
    host: "http://localhost:11434"
    model_id: "qwen3:14b"
    temperature: 0.1
    options:
      num_ctx: 32768 # Context window size (tuned for RTX 4090)

  # ---------------------------------------------------------------------------
  # Anthropic - Claude models
  # ---------------------------------------------------------------------------
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    model_id: "claude-sonnet-4-20250514"
    max_tokens: 4096
    temperature: 0.1

  # ---------------------------------------------------------------------------
  # OpenAI - GPT models
  # ---------------------------------------------------------------------------
  openai:
    api_key: ${OPENAI_API_KEY}
    model_id: "gpt-4o"
    max_tokens: 4096
    temperature: 0.1
    # Optional: base_url for OpenAI-compatible endpoints
    # base_url: "https://api.openai.com/v1"

  # ---------------------------------------------------------------------------
  # AWS Bedrock - Multiple foundation models
  # ---------------------------------------------------------------------------
  bedrock:
    # AWS credentials are loaded from environment or AWS CLI config
    # Set AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION
    model_id: "us.anthropic.claude-sonnet-4-20250514-v1:0"
    region_name: "us-east-1"
    temperature: 0.1
    max_tokens: 4096

  # ---------------------------------------------------------------------------
  # Google Gemini
  # ---------------------------------------------------------------------------
  gemini:
    api_key: ${GOOGLE_API_KEY}
    model_id: "gemini-2.5-flash"
    temperature: 0.1
    max_output_tokens: 4096
    top_p: 0.9

# =============================================================================
# Agent Configuration
# =============================================================================
agent:
  # Tool set to use: "reduced" (19 tools) or "full" (52 tools)
  # Use "reduced" for smaller models, "full" for larger cloud models
  tool_set: reduced

  # Prompt selection (file name without .md extension)
  # Options: system_prompt_minimal (default), system_prompt
  prompt_name: system_prompt_minimal

  # Parallel tool execution helper (internal tool)
  parallel_tool_calls: false

  # Conversation memory management:
  # Option 1: conversation_window - Fixed message count (legacy, default)
  # Option 2: context_budget - Token-based budget (recommended for variable-length outputs)
  #
  # If context_budget is set, it takes precedence over conversation_window.
  # Use context_budget to prevent context overflow from long tool outputs.

  # Legacy: Fixed message count (number of message pairs to keep)
  conversation_window: 5

  # Token budget for conversation history (overrides conversation_window if set)
  # Recommended values by model:
  #   - Ollama (32K context): 16000
  #   - Claude (200K context): 50000
  #   - GPT-4o (128K context): 40000
  context_budget: 16000
