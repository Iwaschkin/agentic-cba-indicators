# =============================================================================
# Example: Ollama Local Configuration (Default)
# =============================================================================
# To use this config:
#   1. Install Ollama: https://ollama.ai
#   2. Pull a model: ollama pull llama3.1
#   3. Start Ollama: ollama serve
#   4. Run: python main.py (uses this config by default)
# =============================================================================

active_provider: ollama

providers:
  ollama:
    host: "http://localhost:11434"
    model_id: "llama3.1:latest"
    temperature: 0.1
    options:
      num_ctx: 16384  # Context window
      
    # Alternative models:
    # model_id: "qwen2.5:14b-instruct"  # Better for larger tool sets
    # model_id: "mistral:latest"

agent:
  # Use reduced tool set for small local models
  tool_set: reduced
  conversation_window: 5
